---
layout: post
comments: true
categories: Probability
published: true
---

<b>Problem 1:</b> Suppose that you have a biased coin (probability $$ p $$ of getting heads). How can you simulate an unbiased coin?

<b>Problem 2:</b> Conversely, suppose that you have a unbiased coin. Can you simulate a coin with probability p of getting heads?

<b>Problem 3:</b> For each of the above, what is the expected number of coin flips before you get the desired "answer"?

---

<b>Solution 1:</b> Suppose that getting a heads then tails is event A, and getting tails then heads is event B. Then these event are disjoint and equiprobable (since $$ \mathbb{P}(A) = p(p-1) = \mathbb{P}(b) $$ ). Hence I flip the coin twice. If I get event $$ A $$ I call it heads. If I get event $$ B $$ I call it tails. Otherwise I repeat until either $$ A $$ or $$ B $$ happens (which it will in finite time since the number of coin flips needed follows a Geometric Progression)

---

<b>Solution 2:</b> This one is a bit trickier. Consider an infinite sequence of coin flips. We will write a number between 0 and 1 expanded in binary. Each digit is decided by a flip of a coin, with it being a 0 if tails or 1 if heads. By letting the number of coin flips go to infinity, we get a uniform distribution $$ X $$ on the unit interval $$ [0,1] $$. Hence we consider the events $$ \{ X \leq p \} $$ and $$ \{ X \geq p \} $$, which have the desired probabilities.
Now notice that if $$ p > 0.5 $$ and the first coin flip is a zero, then X will never be greater than zero. By a reasoning similar to this one (see Solution 3) we deduce that we can know if $$ X \leq p $$ or $$ X \geq p $$ after finitely many times.

---

<b>Solution 3:</b> Let us focus on Problem 2. We can imagine the line segment $$ [0,1] $$ and a dot at point $$ p $$. Everytime we do a coin flip we decide the next digit of our number. In practice this reduces the possible values of $$ X $$ in half, so if the first coin flip equals zero then $$ X \in [0,\frac{1}{2}] $$, and if $$ X $$ equals one then $$ X \in [\frac{1}{2},1] $$. Now if $$ p $$ does not lie in the interval we get then we don't need any more coin flips to know wether $$ X \leq p $$ or $$ X \geq p $$. Therefore at each coin flip there's a $$ 50 \% $$ chance that this will be the last coin flip, so the expected number of throws is $$ \sum^{\infty}_{k=0}{k2^{-k}}=2 $$

For Problem 1 we see that after each pair of throws there is a $$ 2p(1-p) $$ chance that we will be done, so the expected value is $$ \sum^{\infty}_{k=0}{2k(p(1-p))^{-k}} = \frac{2p(1-p)}{(p^2-p+1)^2} $$